{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install google-generativeai\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "QwQG_QVCW3Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud8R8coSQiBo"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import google.generativeai as palm\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u9D9v5lQs4u"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as palm\n",
        "\n",
        "# Set your API key for Google PaLM\n",
        "API_KEY = \"your API_KEY\"\n",
        "palm.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BFzBUrMUX4V",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Ndpf1dUimJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdv-7nzSUo_h"
      },
      "outputs": [],
      "source": [
        "# Define the path to the CSV file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/ADVAI/codebasics_faqs.csv'\n",
        "\n",
        "# Load the CSV file using CSVLoader with a specific encoding\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "loader = CSVLoader(file_path=file_path, source_column='prompt', encoding='ISO-8859-1')  # Use ISO-8859-1 or 'latin1' encoding\n",
        "data = loader.load()\n",
        "\n",
        "# Display the loaded data to confirm it's working\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygj1-PS1Usf-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace with your file path if needed)\n",
        "faq_data = pd.read_csv('/content/drive/MyDrive/ADVAI/codebasics_faqs.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Ensure columns exist\n",
        "assert 'prompt' in faq_data.columns and 'response' in faq_data.columns, \"Dataset must have 'question' and 'answer' columns.\"\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = faq_data['prompt'].tolist()\n",
        "answers = faq_data['response'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc1oSMSQUvqt"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load HuggingFace Instructor Model\n",
        "embedding_model = SentenceTransformer('hkunlp/instructor-xl',device='cpu')\n",
        "\n",
        "# Create embeddings for the FAQ questions\n",
        "question_embeddings = embedding_model.encode(questions, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yh3V27NUz5e"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Initialize FAISS index\n",
        "embedding_dim = question_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Convert to numpy array and add to FAISS index\n",
        "question_embeddings_np = question_embeddings.detach().cpu().numpy()\n",
        "faiss_index.add(question_embeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "vVodlmPLYB_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import google.generativeai as palm\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the dataset\n",
        "faq_data = pd.read_csv('/content/drive/MyDrive/ADVAI/codebasics_faqs.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Ensure columns exist\n",
        "assert 'prompt' in faq_data.columns and 'response' in faq_data.columns, \"Dataset must have 'prompt' and 'response' columns.\"\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = faq_data['prompt'].tolist()\n",
        "answers = faq_data['response'].tolist()\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer('hkunlp/instructor-xl')\n",
        "\n",
        "# Create FAISS index\n",
        "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Create embeddings for the FAQ prompts\n",
        "question_embeddings = embedding_model.encode(questions, convert_to_tensor=True)\n",
        "faiss_index.add(question_embeddings.detach().cpu().numpy())\n",
        "\n",
        "# Configure Google PaLM\n",
        "palm.configure(api_key=\"your-google-palm-api-key\")\n",
        "\n",
        "# Feedback storage (e.g., CSV or a database)\n",
        "FEEDBACK_FILE = \"feedback.csv\"\n",
        "\n",
        "# Function to save feedback\n",
        "def save_feedback(query, response, feedback):\n",
        "    feedback_data = {\n",
        "        \"query\": query,\n",
        "        \"response\": response,\n",
        "        \"feedback\": feedback,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    feedback_df = pd.DataFrame([feedback_data])\n",
        "\n",
        "    # Append feedback to CSV\n",
        "    try:\n",
        "        feedback_df.to_csv(FEEDBACK_FILE, mode='a', header=not open(FEEDBACK_FILE).readlines(), index=False)\n",
        "    except FileNotFoundError:\n",
        "        feedback_df.to_csv(FEEDBACK_FILE, index=False)\n",
        "    st.success(\"Feedback submitted successfully!\")\n",
        "\n",
        "# Streamlit app UI\n",
        "st.title(\"FAQ Assistant with Generative AI\")\n",
        "\n",
        "query = st.text_input(\"Ask a question:\")\n",
        "\n",
        "if query:\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query], convert_to_tensor=True).detach().cpu().numpy()\n",
        "\n",
        "    # Search for the closest match in FAISS\n",
        "    D, I = faiss_index.search(query_embedding, k=1)\n",
        "    best_match_index = I[0][0]\n",
        "    similarity_score = D[0][0]\n",
        "\n",
        "    # Define a similarity threshold\n",
        "    similarity_threshold = 0.5\n",
        "\n",
        "    if similarity_score < similarity_threshold:\n",
        "        # Fetch and display the corresponding response\n",
        "        answer = answers[best_match_index]\n",
        "        st.write(f\"**Answer:** {answer}\")\n",
        "    else:\n",
        "        # Display a fallback response if no good match is found\n",
        "        answer = \"I don't know the answer to this question. Please try rephrasing.\"\n",
        "        st.write(answer)\n",
        "\n",
        "    # Feedback buttons\n",
        "    st.write(\"Was this response helpful?\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        if st.button(\"Good\"):\n",
        "            save_feedback(query, answer, \"Good\")\n",
        "    with col2:\n",
        "        if st.button(\"Neutral\"):\n",
        "            save_feedback(query, answer, \"Neutral\")\n",
        "    with col3:\n",
        "        if st.button(\"Bad\"):\n",
        "            save_feedback(query, answer, \"Bad\")\n",
        "\n",
        "# Optional: Display feedback data\n",
        "if st.checkbox(\"View Feedback\"):\n",
        "    try:\n",
        "        feedback_df = pd.read_csv(FEEDBACK_FILE)\n",
        "        st.write(feedback_df)\n",
        "    except FileNotFoundError:\n",
        "        st.write(\"No feedback available yet.\")\n",
        "\"\"\"\n",
        "\n",
        "# Save the updated code to `app.py`\n",
        "with open(\"app.py\", \"w\") as file:\n",
        "    file.write(app_code)\n",
        "\n",
        "print(\"Streamlit app saved as `app.py` with feedback functionality.\")\n"
      ],
      "metadata": {
        "id": "DwjyW1F6YFmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Load the dataset\n",
        "faq_data = pd.read_csv('/content/drive/MyDrive/ADVAI/codebasics_faqs.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Ensure columns exist\n",
        "assert 'prompt' in faq_data.columns and 'response' in faq_data.columns, \"Dataset must have 'prompt' and 'response' columns.\"\n",
        "\n",
        "# Extract questions and answers\n",
        "questions = faq_data['prompt'].tolist()\n",
        "answers = faq_data['response'].tolist()\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer('hkunlp/instructor-xl')\n",
        "\n",
        "# Create embeddings for the FAQ prompts\n",
        "print(\"Generating embeddings for FAQ questions...\")\n",
        "question_embeddings = embedding_model.encode(questions, convert_to_tensor=False)\n",
        "\n",
        "# Display embeddings for each question\n",
        "print(\"FAQ Question Embeddings:\")\n",
        "for i, (question, embedding) in enumerate(zip(questions, question_embeddings)):\n",
        "    print(f\"Question {i+1}: {question}\")\n",
        "    print(f\"Embedding: {embedding[:5]}... (truncated for display)\")  # Display first 5 values for readability\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Create FAISS index\n",
        "embedding_dim = question_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Add embeddings to FAISS index\n",
        "question_embeddings_np = question_embeddings\n",
        "faiss_index.add(question_embeddings_np)\n",
        "\n",
        "# Simulate a query and display its embedding\n",
        "query = \"How do I reset my password?\"\n",
        "print(\"\\nUser Query:\", query)\n",
        "\n",
        "# Generate embedding for the query\n",
        "query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
        "\n",
        "# Display query embedding\n",
        "print(\"Query Embedding:\")\n",
        "print(query_embedding[0][:5], \"... (truncated for display)\")\n",
        "\n",
        "# Search for the closest match in FAISS\n",
        "D, I = faiss_index.search(query_embedding, k=1)\n",
        "best_match_index = I[0][0]\n",
        "similarity_score = D[0][0]\n",
        "\n",
        "# Display the closest match and similarity score\n",
        "print(\"\\nClosest Match:\")\n",
        "print(f\"FAQ Question: {questions[best_match_index]}\")\n",
        "print(f\"Similarity Score: {similarity_score}\")\n"
      ],
      "metadata": {
        "id": "3IGhileQ55Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('hkunlp/instructor-xl')  # Lightweight model\n",
        "expected = \"No\"\n",
        "predicted = \"No\"\n",
        "\n",
        "# Get embeddings\n",
        "expected_embedding = model.encode(expected, convert_to_tensor=True)\n",
        "predicted_embedding = model.encode(predicted, convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity = util.pytorch_cos_sim(expected_embedding, predicted_embedding)\n",
        "print(f\"Similarity Score: {similarity.item():.2f}\")\n"
      ],
      "metadata": {
        "id": "1bvDmRtM9_7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/ADVAI/codebasics_faqs.csv'  # Replace with your file path\n",
        "faq_data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(faq_data.isnull().sum())\n",
        "\n",
        "# Set up the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(faq_data.isnull(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Heatmap of Missing Data\")\n",
        "plt.xlabel(\"Columns in Dataset\")\n",
        "plt.ylabel(\"Rows\")\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CJ80ZkgiToug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example test set\n",
        "test_queries = [\"How do I reset my password?\", \"How to update my profile?\"]\n",
        "expected_answers = [\"You can reset your password by clicking on 'Forgot Password'.\",\n",
        "                    \"To update your profile, go to settings.\"]\n",
        "\n",
        "# Iterate through test queries\n",
        "for i, test_query in enumerate(test_queries):\n",
        "    # Generate query embedding\n",
        "    query_embedding = embedding_model.encode([test_query], convert_to_tensor=False)\n",
        "\n",
        "    # Search for closest match\n",
        "    D, I = faiss_index.search(query_embedding, k=1)\n",
        "    best_match_index = I[0][0]\n",
        "    predicted_answer = answers[best_match_index]\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference = nltk.word_tokenize(expected_answers[i].lower())\n",
        "    candidate = nltk.word_tokenize(predicted_answer.lower())\n",
        "    smoothie = SmoothingFunction().method4  # Smoothing for short sentences\n",
        "    bleu_score = sentence_bleu([reference], candidate, smoothing_function=smoothie)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Query: {test_query}\")\n",
        "    print(f\"Expected Answer: {expected_answers[i]}\")\n",
        "    print(f\"Predicted Answer: {predicted_answer}\")\n",
        "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "Xhp6FKvD7fgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJhlBkesU36t"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2Kc2rbIVFFs"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken \"your auth token\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eico-ICeVHdB"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start ngrok with explicit port configuration\n",
        "public_url = ngrok.connect(8051)\n",
        "print(\"Streamlit app is running. Access it via:\", public_url)\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null &\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Start ngrok with explicit port configuration\n",
        "public_url = ngrok.connect(8051)\n",
        "print(\"Streamlit app is running. Access it via:\", public_url)\n",
        "\n",
        "# Run the Streamlit app\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8051\"])\n"
      ],
      "metadata": {
        "id": "mbWD_MOEGGKQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}